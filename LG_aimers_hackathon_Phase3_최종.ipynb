{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45318c40-9e91-4a1c-9154-f31510b7eebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1963e1e-f99c-40d8-986d-7b9204d853f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'TRAIN_WINDOW_SIZE':90, # 90일치로 학습\n",
    "    'PREDICT_SIZE':21, # 21일치 예측\n",
    "    'EPOCHS': 1,\n",
    "    'LEARNING_RATE':1e-4,\n",
    "    'BATCH_SIZE':4096,\n",
    "    'SEED':41\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c845712-c43a-414a-8e60-3558effa20b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d14da135-aae3-4ad4-bc75-4ff52b42ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv').drop(columns=['ID', '제품'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dff171fd-d5ee-4d4d-930d-76a8d84e8bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 브랜드 키워드 언급량 가중치\n",
    "keyword = pd.read_csv('brand_keyword_cnt.csv').drop(columns=['브랜드'])\n",
    "keyword = keyword.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cdb45e2-e0c0-479e-bbf3-1466af18688f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb25312da694935935f612dcc6ee224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3822 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 브랜드 키워드 언급량이 0과 1사이 값을 갖도록 조정\n",
    "for idx in tqdm(range(len(keyword))):\n",
    "    maxi = np.max(keyword.iloc[idx,:])\n",
    "    mini = np.min(keyword.iloc[idx,:])\n",
    "    if maxi == mini :\n",
    "        keyword.iloc[idx,:] = 0\n",
    "    else:\n",
    "        keyword.iloc[idx,:] = (keyword.iloc[idx,:] - mini) / (maxi - mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82b6e7f6-4133-40e2-a908-39b2bd50bac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시그모이드 함수 정의\n",
    "def sigmoid(x):\n",
    "    return round(1 / (1 + np.exp(-x)) - 0.2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7ce18db-14b1-40ea-b916-536577759e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_df = keyword.transform(sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fedad3fc-881a-43ae-9132-499d9c978e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 판매금액 정보\n",
    "matching = pd.read_csv('./train.csv')['브랜드']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "359253f1-3614-4e84-bc1b-a6d618ff45cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    1    1 ... 2894 2894 2894]\n",
      "28894\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(matching)\n",
    "matching = label_encoder.transform(matching)\n",
    "print(matching)\n",
    "print(len(matching))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "993b835f-05d0-4bb9-a777-f73bfa202d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "qty = pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "887a1cb9-f406-48e6-8c97-e3b4011dff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_w = pd.read_csv('./sales.csv').drop(columns=['ID', '제품'])\n",
    "sales_w = sales_w.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa455959-1165-4fa1-9ff6-891f99b12252",
   "metadata": {},
   "outputs": [],
   "source": [
    "qty_data = qty.drop(['대분류', '소분류', '중분류', '브랜드', '쇼핑몰'], axis=1).sum(axis=1)\n",
    "sales_data = sales_w.drop(['대분류', '소분류', '중분류', '브랜드', '쇼핑몰'], axis=1).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "135bfdc4-d55c-4c8a-a278-2e864cf5ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_weights = sales_data / qty_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4a6287a-e780-4202-be32-f5f1db2c2016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PMC를 100으로 잡고 100 이상일 때 역수로 가중치를 취함 (제품이 비쌀수록 수요가 적을 것이라 생각함)\n",
    "sales_weights = sales_weights.apply(lambda avg_price: 1 / avg_price if avg_price >= 100 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17beb73-cfc1-40f9-95b3-071ade72c20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ddb921b4dcf4f89a74b01aa50d3753e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28894 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train data에 가중치를 줌\n",
    "for i in tqdm(range(len(train_data))):\n",
    "    for j in range(5, len(train_data.columns)):\n",
    "        mul = sales_weights[i] + sigmoid_df.loc[matching[i], sigmoid_df.columns[j-5]]\n",
    "        w = train_data.loc[i, train_data.columns[j]] * mul\n",
    "        train_data.loc[i, train_data.columns[j]] = round(train_data.loc[i, train_data.columns[j]] + w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6b9dfe-d606-4fbd-bcb7-69f41652336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefed07c-756e-4a4e-86fc-e2cc5a02212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 먼저 5번째 열부터의 데이터만 선택합니다.\n",
    "data_subset = train_data.iloc[:, 5:]\n",
    "\n",
    "# 각 행의 최댓값과 최솟값을 계산합니다.\n",
    "max_values = data_subset.max(axis=1)\n",
    "min_values = data_subset.min(axis=1)\n",
    "\n",
    "# 분모가 0이 되는 경우를 처리하기 위해 조건을 적용합니다.\n",
    "diff = max_values - min_values\n",
    "mask = diff != 0\n",
    "\n",
    "# 정규화 작업을 수행합니다.\n",
    "train_data.loc[mask, data_subset.columns] = (data_subset[mask] - min_values[mask].values.reshape(-1, 1)) / diff[mask].values.reshape(-1, 1)\n",
    "train_data.loc[~mask, data_subset.columns] = 0\n",
    "\n",
    "# 결과를 사전에 저장합니다.\n",
    "scale_max_dict = max_values.to_dict()\n",
    "scale_min_dict = min_values.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78c22b3-c8c8-4fc4-a6a1-376cab73732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = ['대분류', '중분류', '소분류', '브랜드', '쇼핑몰']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    label_encoder.fit(train_data[col])\n",
    "    train_data[col] = label_encoder.transform(train_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e9af1c-27bd-4727-9f25-30a1dfd5aa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, train_size=CFG['TRAIN_WINDOW_SIZE'], predict_size=CFG['PREDICT_SIZE'], is_inference=False):\n",
    "        self.data = data.values # convert DataFrame to numpy array\n",
    "        self.train_size = train_size\n",
    "        self.predict_size = predict_size\n",
    "        self.window_size = self.train_size + self.predict_size\n",
    "        self.is_inference = is_inference\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.is_inference:\n",
    "            return len(self.data)\n",
    "        else:\n",
    "            return self.data.shape[0] * (self.data.shape[1] - self.window_size - 4)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_inference:\n",
    "            # 추론 시\n",
    "            encode_info = self.data[idx, :5]\n",
    "            window = self.data[idx, -self.train_size:]\n",
    "            input_data = np.column_stack((np.tile(encode_info, (self.train_size, 1)), window))\n",
    "            return input_data\n",
    "        else:\n",
    "            # 학습 시\n",
    "            row = idx // (self.data.shape[1] - self.window_size - 4)\n",
    "            col = idx % (self.data.shape[1] - self.window_size - 4)\n",
    "            encode_info = self.data[row, :5]\n",
    "            sales_data = self.data[row, 5:]\n",
    "            window = sales_data[col : col + self.window_size]\n",
    "            input_data = np.column_stack((np.tile(encode_info, (self.train_size, 1)), window[:self.train_size]))\n",
    "            target_data = window[self.train_size:]\n",
    "            return input_data, target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f298e0b2-9833-422c-aab6-5e47e36a83dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomDataset 인스턴스 생성\n",
    "dataset = CustomDataset(train_data)\n",
    "\n",
    "# 전체 데이터셋의 크기\n",
    "total_size = len(dataset)\n",
    "\n",
    "# 분리할 데이터셋의 크기 계산\n",
    "train_size = int(total_size * 0.8)\n",
    "val_size = total_size - train_size\n",
    "\n",
    "# random_split 함수를 사용해 데이터셋 분리\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoader 인스턴스 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92b9085-34fa-47c6-8395-f2a399087ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model과 GRU Model을 Stacking Ensemble한 모델 선언\n",
    "class LSTMModel(nn.Module): # LSTM Model\n",
    "    def __init__(self, input_size=5, hidden_size=512, output_size=CFG['PREDICT_SIZE']):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_size//2, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        output = self.fc(lstm_out[:, -1, :])\n",
    "        return output\n",
    "\n",
    "class GRUModel(nn.Module): # GRU Model\n",
    "    def __init__(self, input_size=5, hidden_size=512, output_size=CFG['PREDICT_SIZE']):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_size//2, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        output = self.fc(gru_out[:, -1, :])\n",
    "        return output\n",
    "\n",
    "lstm_model = LSTMModel()\n",
    "gru_model = GRUModel()\n",
    "\n",
    "class StackingEnsembleModel(nn.Module): # Stacking Ensemble Model\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(StackingEnsembleModel, self).__init__()\n",
    "        self.lstm_model = LSTMModel(input_size, hidden_size, output_size)\n",
    "        self.gru_model = GRUModel(input_size, hidden_size, output_size)\n",
    "        self.ensemble_fc = nn.Sequential(\n",
    "            nn.Linear(output_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_output = self.lstm_model(x)\n",
    "        gru_output = self.gru_model(x)\n",
    "\n",
    "        combined_output = torch.cat((lstm_output, gru_output), dim=1)\n",
    "        ensemble_output = self.ensemble_fc(combined_output)\n",
    "\n",
    "        return ensemble_output\n",
    "\n",
    "output_size = CFG['PREDICT_SIZE']\n",
    "ensemble_model = StackingEnsembleModel(input_size=5, hidden_size=256, output_size=output_size)\n",
    "print(ensemble_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62129eaa-c39f-4526-9e98-8ce68f36c68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    best_loss = 9999999\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        train_mae = []\n",
    "        for X, Y in tqdm(iter(train_loader)):\n",
    "            X = X.float().to(device)\n",
    "            Y = Y.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        val_loss = validation(model, val_loader, criterion, device)\n",
    "        print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : [{val_loss:.5f}]')\n",
    "\n",
    "        if best_loss > val_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "            print('Model Saved')\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10804e6e-3235-4360-9554-ccd58d5f5672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, Y in tqdm(iter(val_loader)):\n",
    "            X = X.float().to(device)\n",
    "            Y = Y.float().to(device)\n",
    "\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "    return np.mean(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2e92a9-f4c9-4a1e-96d8-0664b45aa0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ensemble_model\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a482b1-7d8c-482a-9a6a-eee28404291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(data=train_data, is_inference=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac335531-b01e-4dbe-b950-4dcf211967ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X in tqdm(iter(test_loader)):\n",
    "            X = X.float().to(device)\n",
    "\n",
    "            output = model(X)\n",
    "\n",
    "            # 모델 출력인 output을 CPU로 이동하고 numpy 배열로 변환\n",
    "            output = output.cpu().numpy()\n",
    "\n",
    "            predictions.extend(output)\n",
    "\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1464c499-5508-41e4-9c75-e0809584d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = inference(infer_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e7647-3dee-45de-ab78-6199b3aa3af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론 결과를 inverse scaling\n",
    "for idx in range(len(pred)):\n",
    "    pred[idx, :] = pred[idx, :] * (scale_max_dict[idx] - scale_min_dict[idx]) + scale_min_dict[idx]\n",
    "\n",
    "# 결과 후처리\n",
    "pred = np.round(pred, 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe95e3-144e-4c65-8c1d-f060f3a034fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f6b362-9bb3-45d6-96c0-0d14f712023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8686348b-05bc-40f8-80a4-ad831e46d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.iloc[:,1:] = pred\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816fbc2e-cc8c-4faf-a148-e6d2ae9e14f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('final.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
